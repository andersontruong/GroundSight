{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9a278c-78f5-40c6-8289-b30c0f2500be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9f02d5-1d82-445e-83bf-65fc332f09d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "plt.imshow(train_images[2], cmap='gray')\n",
    "\n",
    "test_image = train_images[2]\n",
    "\n",
    "train_images = train_images.astype(np.float32) / 255\n",
    "test_images = test_images.astype(np.float32) / 255\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "test_images = np.expand_dims(test_images, -1)\n",
    "\n",
    "print(train_labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53418d9f-c2e0-4c16-940f-494d2dd327b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 26, 26, 6)         60        \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 13, 13, 6)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 11, 11, 6)         330       \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 726)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                7270      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,660\n",
      "Trainable params: 7,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flami\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.3050 - accuracy: 0.9114 - val_loss: 0.1362 - val_accuracy: 0.9620\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1115 - accuracy: 0.9657 - val_loss: 0.0906 - val_accuracy: 0.9747\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0834 - accuracy: 0.9751 - val_loss: 0.0837 - val_accuracy: 0.9749\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0690 - accuracy: 0.9789 - val_loss: 0.0731 - val_accuracy: 0.9783\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0619 - accuracy: 0.9809 - val_loss: 0.0659 - val_accuracy: 0.9806\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0534 - accuracy: 0.9832 - val_loss: 0.0681 - val_accuracy: 0.9799\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0491 - accuracy: 0.9849 - val_loss: 0.0624 - val_accuracy: 0.9820\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0454 - accuracy: 0.9858 - val_loss: 0.0695 - val_accuracy: 0.9796\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0414 - accuracy: 0.9873 - val_loss: 0.0609 - val_accuracy: 0.9821\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0384 - accuracy: 0.9880 - val_loss: 0.0620 - val_accuracy: 0.9822\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0358 - accuracy: 0.9884 - val_loss: 0.0597 - val_accuracy: 0.9833\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0329 - accuracy: 0.9897 - val_loss: 0.0597 - val_accuracy: 0.9833\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0314 - accuracy: 0.9898 - val_loss: 0.0616 - val_accuracy: 0.9825\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.0594 - val_accuracy: 0.9822\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0278 - accuracy: 0.9916 - val_loss: 0.0581 - val_accuracy: 0.9833\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.0633 - val_accuracy: 0.9825\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0252 - accuracy: 0.9918 - val_loss: 0.0640 - val_accuracy: 0.9833\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.0688 - val_accuracy: 0.9821\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.0217 - accuracy: 0.9926 - val_loss: 0.0672 - val_accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.0204 - accuracy: 0.9930 - val_loss: 0.0669 - val_accuracy: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23614cc2d10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "input_shape = (28, 28, 1)\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(6, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(6, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f986514-dc02-4400-9dca-2615ab63278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0588 - accuracy: 0.9844\n",
      "Test accuracy: 0.9843999743461609\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86f285db-1418-4c75-ac3f-926910697da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 26, 26, 6)         60        \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 13, 13, 6)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 11, 11, 6)         330       \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 726)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                7270      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,660\n",
      "Trainable params: 7,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0588 - accuracy: 0.9844\n",
      "Restored model, accuracy: 98.44%\n",
      "Restored model, loss: 0.05882437899708748\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "print(\"Restored model, loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dfc59ef1-8be0-475f-85c0-254346c91cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-tflite\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-tflite\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('mnist-tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9202f94-c256-4434-bd6d-c8cd89f3dc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model is 11592 bytes\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('mnist-tflite')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_size = open('./mnist_model.tflite', \"wb\").write(tflite_model)\n",
    "print(\"Quantized model is %d bytes\" % tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b92711-3ebb-4c94-9f28-63af3f290179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/lite/microcontrollers/build_convert\n",
    "# xxd -i mnist_model.tflite > model_data.cc\n",
    "\n",
    "# Helpful for Git Bash https://gist.github.com/evanwill/0207876c3243bbb6863e65ec5dc3f058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9220835-3330-4669-a13a-ca3b94020613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_input_3:0\n",
      "shape: [ 1 28 28  1]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.float32'>\n",
      "\n",
      "DUMP INPUT\n",
      "{'name': 'serving_default_input_3:0', 'index': 0, 'shape': array([ 1, 28, 28,  1]), 'shape_signature': array([-1, 28, 28,  1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "\n",
      "DUMP OUTPUT\n",
      "{'name': 'StatefulPartitionedCall:0', 'index': 15, 'shape': array([ 1, 10]), 'shape_signature': array([-1, 10]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='./mnist_model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
    "\n",
    "print(\"\\nDUMP INPUT\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"\\nDUMP OUTPUT\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9602d497-b3f9-4409-a5fc-e26d9232d730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24608"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\".\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "# Save the quantized model:\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9af0604-5f85-4744-97e2-cedaadbd8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"../tensorflow-lite-esp32/firmware/src/digit.h\",\"w\")\n",
    "f.write(\"const float digit[] = { \\n\")\n",
    "for d in test_image.flatten():\n",
    "    f.write(str(d))\n",
    "    f.write(',')\n",
    "f.write('\\n};')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798540f-69b3-4fea-b005-7bde6193cea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
